# @package _group_

common:
  fp16: true
  log_format: json
  log_interval: 200
  tensorboard_logdir: tb
  fp16_no_flatten_grads: true

checkpoint:
  save_interval: 1
  save_interval_updates: 25000
  keep_interval_updates: 1
  no_epoch_checkpoints: true
  best_checkpoint_metric: accuracy
  maximize_best_checkpoint_metric: true

task:
  _name: mae_image_classification
  data: /datasets01/imagenet_full_size/061417

dataset:
  num_workers: 6
  batch_size: 32
  skip_invalid_size_inputs_valid_test: true
  required_batch_size_multiple: 2
  valid_subset: val

distributed_training:
  distributed_world_size: 16
  ddp_backend: c10d

criterion:
  _name: model
  log_keys:
    - correct

optimization:
  max_update: 250200
  lr: [0.0015]

optimizer:
  _name: composite
  groups:
    with_decay:
      lr_float: 1e-3
      optimizer:
        _name: adam
        adam_betas: [0.9,0.95]
        weight_decay: 0.05
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6
    no_decay:
      lr_float: 1e-3
      optimizer:
        _name: adam
        adam_betas: [0.9,0.95]
        weight_decay: 0
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6

    layer_0_with_decay:
      lr_float: 0.000006697205891
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0.05
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6
    layer_0_no_decay:
      lr_float: 0.000006697205891
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6

    layer_1_with_decay:
      lr_float: 0.000010688009063
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0.05
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6
    layer_1_no_decay:
      lr_float: 0.000010688009063
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6

    layer_2_with_decay:
      lr_float: 0.000016750783174
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0.05
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6
    layer_2_no_decay:
      lr_float: 0.000016750783174
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6

    layer_3_with_decay:
      lr_float: 0.00002346274334
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0.05
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6
    layer_3_no_decay:
      lr_float: 0.00002346274334
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6

    layer_4_with_decay:
      lr_float: 0.00004071191284
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0.05
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6
    layer_4_no_decay:
      lr_float: 0.00004071191284
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6

    layer_5_with_decay:
      lr_float: 0.00006186448129
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0.05
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6
    layer_5_no_decay:
      lr_float: 0.00006186448129
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6

    layer_6_with_decay:
      lr_float: 0.00008902227891
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0.05
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6
    layer_6_no_decay:
      lr_float: 0.00008902227891
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6

    layer_7_with_decay:
      lr_float: 0.00014541889063
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0.05
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6
    layer_7_no_decay:
      lr_float: 0.00014541889063
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6

    layer_8_with_decay:
      lr_float: 0.0002160290625
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0.05
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6
    layer_8_no_decay:
      lr_float: 0.0002160290625
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6

    layer_9_with_decay:
      lr_float: 0.00027850625
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0.05
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6
    layer_9_no_decay:
      lr_float: 0.00027850625
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6

    layer_10_with_decay:
      lr_float: 0.000474625
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0.05
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6
    layer_10_no_decay:
      lr_float: 0.000474625
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6

    layer_11_with_decay:
      lr_float: 0.0008225
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0.05
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6
    layer_11_no_decay:
      lr_float: 0.0008225
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6

    layer_12_with_decay:
      lr_float: 0.00125
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0.05
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6
    layer_12_no_decay:
      lr_float: 0.00125
      optimizer:
        _name: adam
        adam_betas: [ 0.9,0.95 ]
        weight_decay: 0
      lr_scheduler:
        _name: cosine
        warmup_updates: 12510
        min_lr: 1e-6

lr_scheduler: pass_through

model:
  _name: mae_image_classification
  model_path: ???
