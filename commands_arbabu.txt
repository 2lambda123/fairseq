Command to run audo pretraining:

PYTHONPATH=.:/private/home/abaevski/fairseq-py/examples/data2vec PREFIX=d2v/libri/mae_200k_proj nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/audio/pretraining --config-name base_librispeech_dual_mae hydra/launcher=depedency_submitit_slurm +next_script=/private/home/abaevski/fairseq-py-dev/w2v/finetune_w2v_10h_last_mae.sh +run_config=slurm_2 distributed_training.distributed_port=$[${RANDOM}%30000+30000] distributed_training.distributed_world_size=16 &

Command to run vision pre-training:

PYTHONPATH=.:/private/home/abaevski/fairseq-py/examples PREFIX=d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2 distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/private/home/abaevski/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000

Vision finetuning:

PYTHONPATH=.:/private/home/abaevski/fairseq-py/examples PREFIX=d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/private/home/abaevski/fairseq-py/examples/data2vec common.fp16=true model.model_path=$cp/checkpoints/checkpoint_last.pt distributed_training.find_unused_parameters=true +task.rebuild_batches=true &



=====

vision local command

 PYTHONPATH=. python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet  common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 distributed_training.distributed_world_size=1 +model.decoder_type=conv2d dataset.num_workers=0


 PYTHONPATH=. PREFIX=d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 &


 PYTHONPATH=. PREFIX=d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=1&


 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet&

 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=1 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=2 hydra.launcher.partition=wav2vec&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=1 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=3 hydra.launcher.partition=wav2vec&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=1 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=5 hydra.launcher.partition=wav2vec&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=1 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=7 hydra.launcher.partition=wav2vec&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=1 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=9 hydra.launcher.partition=wav2vec&

 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=1 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=1 hydra.launcher.partition=wav2vec optimization.max_update=200000&

 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 +task.local_cache_path=/scratch/cache/imagenet optimization.max_update=200000&


 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 +task.local_cache_path=/scratch/cache/imagenet&


 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:1/+model.mae_conv_decoder_layers:1/model.ema_anneal_end_step:75000/optimization.max_update:200000/checkpoints/checkpoint_160_200000.pt'" &

 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:9/+model.mae_conv_decoder_layers:1/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:7/+model.mae_conv_decoder_layers:1/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:3/+model.mae_conv_decoder_layers:1/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_layers:2/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:5/+model.mae_conv_decoder_layers:1/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:2/+model.mae_conv_decoder_layers:1/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &




//done 
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=1 +task.local_cache_path=/scratch/cache/imagenet&

 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_layers:1/model.ema_anneal_end_step:75000/checkpoints/checkpoint_last.pt'" &

// round 2

 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=20000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=4 hydra.launcher.partition=wav2vec&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=8 hydra.launcher.partition=wav2vec&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=12 hydra.launcher.partition=wav2vec&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=16 hydra.launcher.partition=wav2vec&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=1 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=8 hydra.launcher.partition=wav2vec&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=1 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=12 hydra.launcher.partition=wav2vec&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=1 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=16 hydra.launcher.partition=wav2vec&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=3 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec&
  

 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:4/+model.mae_conv_decoder_layers:2/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:8/+model.mae_conv_decoder_layers:1/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:8/+model.mae_conv_decoder_layers:2/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:12/+model.mae_conv_decoder_layers:1/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:16/+model.mae_conv_decoder_layers:1/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:12/+model.mae_conv_decoder_layers:2/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &

 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/model.ema_anneal_end_step:20000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:16/+model.mae_conv_decoder_layers:2/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
   
//

 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=4 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec +model.mae_conv_decoder_residual=true&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=5 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec +model.mae_conv_decoder_residual=true&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=6 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec +model.mae_conv_decoder_residual=true&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=7 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec +model.mae_conv_decoder_residual=true&


 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec optimization.lr=[0.0008] &
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec optimization.lr=[0.00045] &

 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec model.average_top_k_layers=3&
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec model.average_top_k_layers=5&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec model.average_top_k_layers=6&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec model.average_top_k_layers=8&

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 model.mask_ratio=0.70 hydra.launcher.partition=wav2vec&

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 model.mask_ratio=0.85 hydra.launcher.partition=wav2vec&

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 model.layer_norm_target_layer=false model.layer_norm_targets=true hydra.launcher.partition=wav2vec&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet +model.mae_conv_decoder_kernel=6 model.layer_norm_target_layer=true model.layer_norm_targets=true hydra.launcher.partition=wav2vec&


 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:3/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/model.average_top_k_layers:3/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/model.average_top_k_layers:8/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/model.ema_anneal_end_step:75000/model.layer_norm_target_layer:False/model.layer_norm_targets:True/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/model.ema_anneal_end_step:75000/optimization.lr:[0.0008]/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/model.ema_anneal_end_step:75000/optimization.lr:[0.00045]/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:4/+model.mae_conv_decoder_residual:True/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/model.average_top_k_layers:6/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:6/+model.mae_conv_decoder_residual:True/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/model.ema_anneal_end_step:75000/model.mask_ratio:0.85/checkpoints/checkpoint_300_375000.pt'" &

   //
   PYTHONPATH=. python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet  common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 distributed_training.distributed_world_size=1 +model.decoder_type=conv2d dataset.num_workers=0 +model.mask_type=block

   PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet_arbabu +model.mae_conv_decoder_kernel=6  +model.mask_type=block +model.num_mask_patches=75  hydra.launcher.partition=wav2vec&
   PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache_arbabu/imagenet +model.mae_conv_decoder_kernel=6  +model.mask_type=block +model.num_mask_patches=120   hydra.launcher.partition=wav2vec&
   PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache_arbabu/imagenet +model.mae_conv_decoder_kernel=6  +model.mask_type=block +model.num_mask_patches=150   hydra.launcher.partition=wav2vec&

   PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet_arbabu +model.mae_conv_decoder_kernel=6  +model.mask_type=random +model.mae_conv_decoder_groups=4   hydra.launcher.partition=wav2vec&
   PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet_arbabu +model.mae_conv_decoder_kernel=6  +model.mask_type=random +model.mae_conv_decoder_groups=8   hydra.launcher.partition=wav2vec&
   PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet_arbabu +model.mae_conv_decoder_kernel=6  +model.mask_type=random +model.mae_conv_decoder_groups=16   hydra.launcher.partition=wav2vec&
      PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet_arbabu +model.mae_conv_decoder_kernel=6  +model.mask_type=random +model.mae_conv_decoder_groups=24   hydra.launcher.partition=wav2vec&

 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.mae_conv_decoder_groups:16/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/+model.mask_type:random/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.mae_conv_decoder_groups:24/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/+model.mask_type:random/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.mae_conv_decoder_groups:8/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/+model.mask_type:random/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/+model.mask_type:block/+model.num_mask_patches:75/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &


 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/+model.mask_type:block/+model.num_mask_patches:120/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/+model.mask_type:block/+model.num_mask_patches:150/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
   
   PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache_arbabu/imagenet +model.mae_conv_decoder_kernel=6  +model.mask_type=block +model.num_mask_patches=160   hydra.launcher.partition=wav2vec&
   PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache_arbabu/imagenet +model.mae_conv_decoder_kernel=6  +model.mask_type=block +model.num_mask_patches=170   hydra.launcher.partition=wav2vec&
   PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache_arbabu/imagenet +model.mae_conv_decoder_kernel=6  +model.mask_type=block +model.num_mask_patches=180   hydra.launcher.partition=wav2vec&

 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/+model.mask_type:block/+model.num_mask_patches:160/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/+model.mask_type:block/+model.num_mask_patches:180/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/+model.mask_type:block/+model.num_mask_patches:170/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &


 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.mae_conv_decoder_groups:8/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/+model.mask_type:random/model.ema_anneal_end_step:75000/checkpoints/checkpoint_300_375000.pt'" &
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/model.ema_anneal_end_step:75000/model.mask_ratio:0.85/checkpoints/checkpoint_300_375000.pt'" &'
   
//

 PYTHONPATH=. python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet  common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 distributed_training.distributed_world_size=1 +model.decoder_type=conv2d dataset.num_workers=0 +model.clone_batch=2


 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet_arbabu +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec +model.clone_batch=1 dataset.batch_size=64 &

 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache/imagenet_arbabu +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec +model.clone_batch=4 dataset.batch_size=16 &


 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache_arbabu/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec +model.clone_batch=8 dataset.batch_size=8 &
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache_arbabu/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec +model.clone_batch=2 dataset.batch_size=32 &
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache_arbabu/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec +model.clone_batch=2 dataset.batch_size=64 &
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache_arbabu/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec +model.clone_batch=4 dataset.batch_size=48 &


 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache_arbabu/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec +model.clone_batch=3 dataset.batch_size=64 &
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache_arbabu/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec +model.clone_batch=4 dataset.batch_size=64 &

 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache_arbabu/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec +model.clone_batch=4 dataset.batch_size=48 optimization.max_update=500400 &
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache_arbabu/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec +model.clone_batch=6 dataset.batch_size=48 optimization.max_update=500400 &
 PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec \
   model.ema_anneal_end_step=75000 +model.decoder_type=conv2d +model.mae_conv_decoder_layers=2 +task.local_cache_path=/scratch/cache_arbabu/imagenet +model.mae_conv_decoder_kernel=6 hydra.launcher.partition=wav2vec +model.clone_batch=2 dataset.batch_size=48 optimization.max_update=500400 &
   // 48 == 500400

 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.clone_batch:1/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/dataset.batch_size:64/model.ema_anneal_end_step:75000/checkpoints/checkpoint_last.pt'" &


 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.clone_batch:4/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/dataset.batch_size:16/model.ema_anneal_end_step:75000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.clone_batch:8/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/dataset.batch_size:8/model.ema_anneal_end_step:75000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.clone_batch:2/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/dataset.batch_size:32/model.ema_anneal_end_step:75000/checkpoints/checkpoint_last.pt'" &


 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.clone_batch:2/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/dataset.batch_size:64/model.ema_anneal_end_step:75000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.clone_batch:4/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/dataset.batch_size:48/model.ema_anneal_end_step:75000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.clone_batch:3/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/dataset.batch_size:64/model.ema_anneal_end_step:75000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.clone_batch:4/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/dataset.batch_size:64/model.ema_anneal_end_step:75000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.clone_batch:2/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/dataset.batch_size:48/model.ema_anneal_end_step:75000/optimization.max_update:500400/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.clone_batch:4/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/dataset.batch_size:48/model.ema_anneal_end_step:75000/optimization.max_update:500400/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.clone_batch:6/+model.decoder_type:conv2d/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/dataset.batch_size:48/model.ema_anneal_end_step:75000/optimization.max_update:500400/checkpoints/checkpoint_last.pt'" &

 PYTHONPATH=. python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet  common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec model.ema_anneal_end_step=75000 distributed_training.distributed_world_size=1 model.decoder_type=conv2d dataset.num_workers=0 +model.clone_batch=2  +model.mask_type=block +model.num_mask_patches=150 common.log_interval=1
 
ab_config

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=1 dataset.batch_size=64 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=2 dataset.batch_size=32 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=4 dataset.batch_size=16 &

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=1 dataset.batch_size=64 +model.mask_type=block +model.num_mask_patches=170&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=2 dataset.batch_size=32 +model.mask_type=block +model.num_mask_patches=170&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=4 dataset.batch_size=16 +model.mask_type=block +model.num_mask_patches=170&

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=1 dataset.batch_size=64 +model.mask_type=block +model.num_mask_patches=180&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=2 dataset.batch_size=32 +model.mask_type=block +model.num_mask_patches=180&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=4 dataset.batch_size=16 +model.mask_type=block +model.num_mask_patches=180&

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet model.mae_conv_decoder_groups=4 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet model.mae_conv_decoder_groups=8 &

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet model.mae_conv_decoder_layers=6 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet model.mae_conv_decoder_layers=7 &

 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:4/dataset.batch_size:16/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_8/+model.mae_conv_decoder_groups:4/+model.mae_conv_decoder_kernel:6/+model.mae_conv_decoder_layers:2/+model.mask_type:random/model.ema_anneal_end_step:75000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:2/dataset.batch_size:32/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:4/+model.mask_type:block/+model.num_mask_patches:170/dataset.batch_size:16/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/model.mae_conv_decoder_layers:6/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:1/dataset.batch_size:64/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:4/+model.mask_type:block/+model.num_mask_patches:180/dataset.batch_size:16/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:2/+model.mask_type:block/+model.num_mask_patches:170/dataset.batch_size:32/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/model.mae_conv_decoder_groups:4/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:2/+model.mask_type:block/+model.num_mask_patches:180/dataset.batch_size:32/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:1/+model.mask_type:block/+model.num_mask_patches:170/dataset.batch_size:64/checkpoints/checkpoint_last.pt'" &


PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=4 dataset.batch_size=16 +model.mask_type=block +model.num_mask_patches=180 model.mae_conv_decoder_layers=6&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=4 dataset.batch_size=16 +model.mask_type=block +model.num_mask_patches=180 model.mae_conv_decoder_layers=7&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=4 dataset.batch_size=16 +model.mask_type=block +model.num_mask_patches=180 model.mae_conv_decoder_layers=8&

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=4 dataset.batch_size=16 +model.mask_type=block +model.num_mask_patches=185&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=4 dataset.batch_size=16 +model.mask_type=block +model.num_mask_patches=190&

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=4 dataset.batch_size=16 +model.mask_type=block +model.num_mask_patches=180 model.mae_conv_decoder_layers=6 model.mae_conv_decoder_groups=4&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=4 dataset.batch_size=16 +model.mask_type=block +model.num_mask_patches=180 model.mae_conv_decoder_layers=6 model.mae_conv_decoder_groups=8&

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=4 dataset.batch_size=16 +model.mask_type=block +model.num_mask_patches=180 model.mae_conv_decoder_layers=8 model.mae_conv_decoder_groups=32&


 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:1/+model.mask_type:block/+model.num_mask_patches:180/dataset.batch_size:64/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:2/+model.mask_type:block/+model.num_mask_patches:170/dataset.batch_size:32/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
   
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=1 dataset.batch_size=64 +model.mask_type=block +model.num_mask_patches=160 hydra.launcher.partition=wav2vec&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=2 dataset.batch_size=32 +model.mask_type=block +model.num_mask_patches=160 hydra.launcher.partition=wav2vec&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet  +model.clone_batch=4 dataset.batch_size=16 +model.mask_type=block +model.num_mask_patches=160 hydra.launcher.partition=wav2vec&


PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=8 hydra.launcher.partition=wav2vec&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=32 hydra.launcher.partition=wav2vec&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=64 hydra.launcher.partition=wav2vec&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=128 hydra.launcher.partition=wav2vec&


 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:4/+model.mask_type:block/+model.num_mask_patches:180/dataset.batch_size:16/model.mae_conv_decoder_layers:8/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:4/+model.mask_type:block/+model.num_mask_patches:180/dataset.batch_size:16/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:4/+model.mask_type:block/+model.num_mask_patches:185/dataset.batch_size:16/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:8/dataset.batch_size:8/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:4/+model.mask_type:block/+model.num_mask_patches:180/dataset.batch_size:16/model.mae_conv_decoder_layers:6/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:32/dataset.batch_size:8/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_decoderlr_8/+model.clone_batch:1/dataset.batch_size:64/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:1/+model.mask_type:block/+model.num_mask_patches:160/dataset.batch_size:64/checkpoints/checkpoint_last.pt'" &



//

 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:4/+model.mask_type:block/+model.num_mask_patches:180/dataset.batch_size:16/model.mae_conv_decoder_groups:32/model.mae_conv_decoder_layers:8/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:4/+model.mask_type:block/+model.num_mask_patches:180/dataset.batch_size:16/model.mae_conv_decoder_groups:8/model.mae_conv_decoder_layers:6/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:4/+model.mask_type:block/+model.num_mask_patches:190/dataset.batch_size:16/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:128/dataset.batch_size:8/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:4/+model.mask_type:block/+model.num_mask_patches:160/dataset.batch_size:16/checkpoints/checkpoint_last.pt'" &




 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:2/+model.mask_type:block/+model.num_mask_patches:160/dataset.batch_size:32/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:1/+model.mask_type:block/+model.num_mask_patches:180/dataset.batch_size:64/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:4/+model.mask_type:block/+model.num_mask_patches:180/dataset.batch_size:16/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:4/+model.mask_type:block/+model.num_mask_patches:170/dataset.batch_size:16/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:4/+model.mask_type:block/+model.num_mask_patches:170/dataset.batch_size:16/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:128/dataset.batch_size:8/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:64/dataset.batch_size:8/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:8/dataset.batch_size:8/checkpoints/checkpoint_last.pt'" &

   
#increase max_updates baseline: 375300; try 400k; 430k
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec optimization.max_update=400000&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec optimization.max_update=430000&

  mae_conv_decoder_layers: 5
  mae_conv_decoder_kernel: 3
  mae_conv_decoder_groups: 16

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=6 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 &

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_kernel=5 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_kernel=7 &

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_groups=8 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_groups=4 &

 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/optimization.max_update:400000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.mae_conv_decoder_groups:8/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/optimization.max_update:430000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.mae_conv_decoder_kernel:5/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.mae_conv_decoder_layers:6/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.mae_conv_decoder_kernel:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.mae_conv_decoder_groups:4/checkpoints/checkpoint_last.pt'" &

// tune things for clone setup ; note that we change baselines here


PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_75e4 hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_9e4 hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 &

// sweep lr warmup
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_w25 hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_w75 hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 &

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.average_top_k_layers=6&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.average_top_k_layers=8&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.average_top_k_layers=12&

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.mask_ratio=0.75 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.mask_ratio=0.85 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.mask_ratio=0.90 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.mask_ratio=0.95 &


// sweep ema_anneal_end_step

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=10000 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=20000 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=30000 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=40000 &


PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=50000 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=100000 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=150000 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=200000 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=250000 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=300000 &


PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=50000 optimization.max_update=500000&

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=50000 model.mask_ratio=0.90 model.mae_conv_decoder_groups=4&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=50000 model.mask_ratio=0.90 model.mae_conv_decoder_groups=8&

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_9e4 hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=50000 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 &
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_75e4 hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=50000 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 &

===
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.mae_conv_decoder_layers:7/model.mask_ratio:0.9/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.average_top_k_layers:6/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_75e4_8/+model.clone_batch:16/dataset.batch_size:8/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.mae_conv_decoder_layers:7/model.mask_ratio:0.85/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:100000/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.average_top_k_layers:8/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.average_top_k_layers:12/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_w25_8/+model.clone_batch:16/dataset.batch_size:8/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:150000/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:250000/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_w75_8/+model.clone_batch:16/dataset.batch_size:8/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:200000/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:300000/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &


 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.mae_conv_decoder_layers:7/model.mask_ratio:0.95/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:40000/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/model.mask_ratio:0.9/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_groups:4/model.mae_conv_decoder_layers:7/model.mask_ratio:0.9/checkpoints/checkpoint_last.pt'" &
   

//

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.mask_ratio=0.90 model.ema_anneal_end_step=50000&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.mask_ratio=0.90 model.ema_anneal_end_step=75000&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.mask_ratio=0.90 model.ema_anneal_end_step=25000&

PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.mask_ratio=0.90 model.average_top_k_layers=8&
PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.mask_ratio=0.90 model.average_top_k_layers=12&


PYTHONPATH=.:./examples PREFIX=cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_5e4 hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec \
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 &
 
  PYTHONPATH=.:./examples PREFIX=finalfix_cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_lrL hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
 common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec +model.use_rel_pos_bias=true +model.shared_rel_pos_bias=true task.local_cache_path=/scratch/cache_arbabu/imagenet&

  PYTHONPATH=.:./examples PREFIX=finalfix_cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_lrL hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
 common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec +model.use_rel_pos_bias=true +model.shared_rel_pos_bias=False task.local_cache_path=/scratch/cache_arbabu/imagenet&
   PYTHONPATH=.:./examples PREFIX=finalfix_cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_lrL hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
 common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec +model.use_rel_pos_bias=False task.local_cache_path=/scratch/cache_arbabu/imagenet&

 PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/finalfix_cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_lrL_8/+model.shared_rel_pos_bias:True/+model.use_rel_pos_bias:True/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/finalfix_cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_lrL_8/+model.shared_rel_pos_bias:False/+model.use_rel_pos_bias:True/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_clonebatch/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/finalfix_cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_lrL_8/+model.use_rel_pos_bias:False/checkpoints/checkpoint_last.pt'" &

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [2022-08-14 04:58:36,590][submitit][INFO] - Job completed successfully
 PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/model.mask_ratio:0.9/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.average_top_k_layers:8/model.mae_conv_decoder_layers:7/model.mask_ratio:0.9/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:75000/model.mae_conv_decoder_layers:7/model.mask_ratio:0.9/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.average_top_k_layers:12/model.mae_conv_decoder_layers:7/model.mask_ratio:0.9/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_5e4_8/+model.clone_batch:16/dataset.batch_size:8/model.mae_conv_decoder_layers:7/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:25000/model.mae_conv_decoder_layers:7/model.mask_ratio:0.9/checkpoints/checkpoint_last.pt'" &

// command with dependency

PYTHONPATH=.:./examples USER_DIR_OVERRIDE=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py PREFIX=DUMMY_cached_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm +next_script=./finetune_imagenet_arbabu.sh \
    +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] optimization.max_update=10 common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py&

 PYTHONPATH=. PREFIX=testtsettfixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/finalfix_cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_lrL_8/+model.use_rel_pos_bias:False/checkpoints/checkpoint_last.pt'" &

// new rounds of sweeps August 18

// rerun mask ablation

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=64 +model.clone_batch=1 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.mask_ratio=0.70&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=64 +model.clone_batch=1 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.mask_ratio=0.80&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=64 +model.clone_batch=1 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.mask_ratio=0.90&

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=50000 model.ema_decay=0.9992&

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=50000 model.ema_decay=0.9995&

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=50000 model.ema_decay=0.9997&

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=50000 model.ema_end_decay=0.9999&

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=50000 model.ema_end_decay=0.99999&

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 +model.clone_batch=16 hydra.launcher.partition=wav2vec model.mae_conv_decoder_layers=7 model.ema_anneal_end_step=50000 model.layer_norm_target_layer=False&


// large job
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec hydra.launcher.nodes=4 distributed_training.distributed_world_size=32&

 PYTHONPATH=. PREFIX=testtsettfixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/checkpoints/checkpoint_last.pt'" &

PYTHONPATH=.:./examples PREFIX=32cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec hydra.launcher.nodes=4 distributed_training.distributed_world_size=32&

// sweep large jobs


PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=16&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=14&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=18&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=20&

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=18 model.ema_decay=0.999&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=18 model.ema_decay=0.9992&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=18 model.ema_decay=0.9994&

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=18 model.ema_anneal_end_step=10000&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=18 model.ema_anneal_end_step=20000&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=18 model.ema_anneal_end_step=40000&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=18 model.ema_anneal_end_step=60000&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=18 model.ema_anneal_end_step=120000&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=18 model.ema_anneal_end_step=140000&

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=18 model.mask_ratio=0.75&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=18 model.mask_ratio=0.85&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=18 model.mask_ratio=0.90&

//new ab2 config

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_ab2 hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.mask_ratio=0.75&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_ab2 hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.mask_ratio=0.8&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_ab2 hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.mask_ratio=0.85&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_ab2 hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.mask_ratio=0.9&


PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_ab2 hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.mask_ratio=0.75 dataset.batch_size=8 +model.clone_batch=16 &
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_ab2 hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.mask_ratio=0.80 dataset.batch_size=8 +model.clone_batch=16 &
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_ab2 hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.mask_ratio=0.85 dataset.batch_size=8 +model.clone_batch=16 &
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_d2v_mae_imagenet_exp_ab2 hydra/launcher=depedency_submitit_slurm \
   +next_script=./finetune_imagenet_arbabu.sh \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.mask_ratio=0.9 dataset.batch_size=8 +model.clone_batch=16 &

// sweeping droppath rate for the 500k model

PYTHONPATH=. PREFIX=testtsettfixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +model.drop_path_rate=0.05&
PYTHONPATH=. PREFIX=testtsettfixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +model.drop_path_rate=0.15&
PYTHONPATH=. PREFIX=testtsettfixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +model.drop_path_rate=0.2&
PYTHONPATH=. PREFIX=testtsettfixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +model.drop_path_rate=0.25&

PYTHONPATH=. PREFIX=testtsettfixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:1/dataset.batch_size:64/model.mae_conv_decoder_layers:7/model.mask_ratio:0.8/checkpoints/checkpoint_last.pt'" &

PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_ab2_8/model.mask_ratio:0.85/checkpoints/checkpoint_last.pt'" &
PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_ab2_8/model.mask_ratio:0.75/checkpoints/checkpoint_last.pt'" &
PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_ab2_8/model.mask_ratio:0.8/checkpoints/checkpoint_last.pt'" &
PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_ab2_8/model.mask_ratio:0.9/checkpoints/checkpoint_last.pt'" &
PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_ab2_8/+model.clone_batch:16/dataset.batch_size:8/model.mask_ratio:0.85/checkpoints/checkpoint_last.pt'" &
PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_ab2_8/+model.clone_batch:16/dataset.batch_size:8/model.mask_ratio:0.9/checkpoints/checkpoint_last.pt'" &
PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_ab2_8/+model.clone_batch:16/dataset.batch_size:8/model.mask_ratio:0.75/checkpoints/checkpoint_last.pt'" &
PYTHONPATH=. PREFIX=fixed_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_ab2_8/+model.clone_batch:16/dataset.batch_size:8/model.mask_ratio:0.8/checkpoints/checkpoint_last.pt'" &
   
// round2
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=24&

PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4&

PYTHONPATH=.:./examples PREFIX=48_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=48 hydra.launcher.nodes=6&

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 &

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 model.ema_anneal_end_step=60000 &

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 model.ema_anneal_end_step=80000 &

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 model.mask_ratio=0.75 &

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 model.mask_ratio=0.85 &

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 model.mask_ratio=0.90 &

PYTHONPATH=.:./examples PREFIX=3e4_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name huge_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=30&
PYTHONPATH=.:./examples PREFIX=3e4_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name huge_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=28&
PYTHONPATH=.:./examples PREFIX=3e4_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name huge_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=30 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4&

PYTHONPATH=.:./examples PREFIX=3e4_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name huge_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=28 optimization.clip_norm=1&
PYTHONPATH=.:./examples PREFIX=3e4_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name huge_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=28 optimization.clip_norm=1 model.ema_anneal_end_step=60000&

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 dataset.batch_size=8 model.clone_batch=32 &
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 dataset.batch_size=16 model.clone_batch=16 &
// testin composite optimizer changes
 PYTHONPATH=. python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet distributed_training.distributed_world_size=1  +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite/fairseq-py/examples/data2vec  common.fp16=true  "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/finalfix_cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_lrL_8/+model.use_rel_pos_bias:False/checkpoints/checkpoint_last.pt'" 

 PYTHONPATH=. python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean \
   distributed_training.distributed_world_size=1  +task.local_cache_path=/scratch/cache_arbabu/imagenet \
   common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite_4push/fairseq-py/examples/data2vec  common.fp16=true  \
   "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/finalfix_cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_lrL_8/+model.use_rel_pos_bias:False/checkpoints/checkpoint_last.pt'" \
   +model.add_model_overrides=true

 PYTHONPATH=. python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet \
   distributed_training.distributed_world_size=1  +task.local_cache_path=/scratch/cache_arbabu/imagenet \
   common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite_4push/fairseq-py/examples/data2vec  common.fp16=true  \
   "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/finalfix_cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_lrL_8/+model.use_rel_pos_bias:False/checkpoints/checkpoint_last.pt'" \
   +model.add_model_overrides=true

 PYTHONPATH=. PREFIX=refactor_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" &

 PYTHONPATH=. PREFIX=refactor2_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +optimizer.manual_groups=False&
   
 PYTHONPATH=. PREFIX=refactor2_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +optimizer.dynamic_groups=true  +optimizer.manual_groups=False&

 PYTHONPATH=. PREFIX=refactor2_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +optimizer.manual_groups=False optimizer.groups.base.optimizer.lr=[0.0015]&
 PYTHONPATH=. PREFIX=refactor2_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +optimizer.manual_groups=False optimizer.groups.base.optimizer.lr=[0.002]&
 PYTHONPATH=. PREFIX=refactor2_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +optimizer.manual_groups=False optimizer.groups.base.optimizer.lr=[0.0025]&

 PYTHONPATH=. PREFIX=refactor2_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +optimizer.manual_groups=False optimizer.groups.base.optimizer.lr=[0.0015] +model.layer_decay=0.7&
 PYTHONPATH=. PREFIX=refactor2_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +optimizer.manual_groups=False optimizer.groups.base.optimizer.lr=[0.0015] +model.layer_decay=0.6&

 PYTHONPATH=. PREFIX=refactor2_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +optimizer.manual_groups=False optimizer.groups.base.optimizer.lr=[0.001] +model.layer_decay=0.7&
 PYTHONPATH=. PREFIX=refactor2_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +optimizer.manual_groups=False optimizer.groups.base.optimizer.lr=[0.001] +model.layer_decay=0.6&

 PYTHONPATH=. PREFIX=refactor2_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +optimizer.manual_groups=False optimizer.groups.base.optimizer.lr=[0.0008]&
 PYTHONPATH=. PREFIX=refactor2_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +optimizer.manual_groups=False optimizer.groups.base.optimizer.lr=[0.0006]&

// afre final refactor

 PYTHONPATH=. PREFIX=refactor3_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite_4push/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" &

 PYTHONPATH=. PREFIX=refactor3_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite_4push/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=true&

 PYTHONPATH=. PREFIX=refactor3_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite_4push/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" &

 PYTHONPATH=. PREFIX=refactor5_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite_4push/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" \
    +model.add_model_overrides=true&

 PYTHONPATH=. PREFIX=refactor5_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite_4push/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" &

 PYTHONPATH=. PREFIX=refactor6_cached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite_4push/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cached_d2v/vision/d2v_mae2/base_d2v_mae_imagenet_exp_8/+model.clone_batch:16/dataset.batch_size:8/model.ema_anneal_end_step:50000/model.mae_conv_decoder_layers:7/optimization.max_update:500000/checkpoints/checkpoint_last.pt'" \
    +model.add_model_overrides=true&

//
 PYTHONPATH=. python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean \
   distributed_training.distributed_world_size=1  +task.local_cache_path=/scratch/cache_arbabu/imagenet \
   common.user_dir=/data/home/arbabu/d2v_working_PUSH_composite_4push/fairseq-py/examples/data2vec  common.fp16=true  \
   "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/checkpoints/checkpoint_last.pt'" \
   +model.add_model_overrides=true


 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100&

 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 optimizer.groups.default.optimizer.lr=[0.0015]&
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 optimizer.groups.default.optimizer.lr=[0.002]&


PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 optimizer.groups.encoder_with_decay.lr_float=2e-4 optimizer.groups.encoder_no_decay.lr_float=2e-4 optimizer.groups.decoder_with_decay.lr_float=2e-4 optimizer.groups.decoder_no_decay.lr_float=2e-4&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 optimizer.groups.encoder_with_decay.lr_float=3e-4 optimizer.groups.encoder_no_decay.lr_float=3e-4 optimizer.groups.decoder_with_decay.lr_float=3e-4 optimizer.groups.decoder_no_decay.lr_float=3e-4&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 optimizer.groups.encoder_with_decay.lr_float=1e-4 optimizer.groups.encoder_no_decay.lr_float=1e-4 optimizer.groups.decoder_with_decay.lr_float=1e-4 optimizer.groups.decoder_no_decay.lr_float=1e-4&


 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 +model.layer_decay=0.75&

 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 +model.drop_path_rate=0.2&


 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=75060 &
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=100080 &

 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 +model.layer_decay=0.65&
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 +model.layer_decay=0.60&
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 +model.layer_decay=0.70&

 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 +model.drop_path_rate=0.2&
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 +model.drop_path_rate=0.15&
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 +model.drop_path_rate=0.1&
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 +model.drop_path_rate=0.05&

PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 model.clone_batch=8 dataset.batch_size=8&
PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 model.clone_batch=32 dataset.batch_size=8&

PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 model.clone_batch=16 dataset.batch_size=4&
PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 model.clone_batch=32 dataset.batch_size=4&
PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 model.clone_batch=64 dataset.batch_size=4&

PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 model.clone_batch=16 dataset.batch_size=16&
PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 model.clone_batch=8 dataset.batch_size=16&
 
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/dataset.batch_size:8/model.average_top_k_layers:22/model.clone_batch:8/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/dataset.batch_size:4/model.average_top_k_layers:22/model.clone_batch:32/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/dataset.batch_size:4/model.average_top_k_layers:22/model.clone_batch:16/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &


 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/optimization.max_update:750600/optimizer.groups.decoder_no_decay.lr_float:0.0001/optimizer.groups.decoder_with_decay.lr_float:0.0001/optimizer.groups.encoder_no_decay.lr_float:0.0001/optimizer.groups.encoder_with_decay.lr_float:0.0001/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/optimization.max_update:750600/optimizer.groups.decoder_no_decay.lr_float:0.0002/optimizer.groups.decoder_with_decay.lr_float:0.0002/optimizer.groups.encoder_no_decay.lr_float:0.0002/optimizer.groups.encoder_with_decay.lr_float:0.0002/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/optimization.max_update:750600/optimizer.groups.decoder_no_decay.lr_float:0.0003/optimizer.groups.decoder_with_decay.lr_float:0.0003/optimizer.groups.encoder_no_decay.lr_float:0.0003/optimizer.groups.encoder_with_decay.lr_float:0.0003/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/dataset.batch_size:16/model.average_top_k_layers:22/model.clone_batch:8/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/dataset.batch_size:4/model.average_top_k_layers:22/model.clone_batch:64/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/dataset.batch_size:8/model.average_top_k_layers:22/model.clone_batch:32/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=reprocached_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/dataset.batch_size:16/model.average_top_k_layers:22/model.clone_batch:16/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
   

 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/dataset.batch_size:16/model.average_top_k_layers:22/model.clone_batch:8/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &

 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/dataset.batch_size:16/model.average_top_k_layers:22/model.clone_batch:8/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 optimizer.groups.default.optimizer.adam_betas.1=0.98&

 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/dataset.batch_size:16/model.average_top_k_layers:22/model.clone_batch:8/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 optimizer.groups.default.optimizer.adam_betas.1=0.999&

 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/dataset.batch_size:16/model.average_top_k_layers:22/model.clone_batch:8/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 optimizer.groups.default.optimizer.adam_betas.1=0.9&
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/dataset.batch_size:16/model.average_top_k_layers:22/model.clone_batch:8/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 optimizer.groups.default.optimizer.adam_betas.1=0.94&
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/dataset.batch_size:16/model.average_top_k_layers:22/model.clone_batch:8/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 optimizer.groups.default.optimizer.adam_betas.1=0.96&


    +model.drop_path_rate=0.05&
   

// mega ema sweep for large model 600 epochs
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 model.ema_decay=0.999&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 model.ema_decay=0.9992&
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 model.ema_decay=0.9994&

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 model.ema_end_decay=0.99999&

PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 model.ema_anneal_end_step=25000 &
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 model.ema_anneal_end_step=50000 &
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 model.ema_anneal_end_step=100000 &
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 model.ema_anneal_end_step=125000 &
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 model.ema_anneal_end_step=150000 &
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 model.ema_anneal_end_step=175000 &
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 model.ema_anneal_end_step=200000 &
PYTHONPATH=.:./examples PREFIX=cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 optimization.max_update=750600 model.ema_anneal_end_step=250000 &

PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32  hydra.launcher.nodes=4 model.layer_norm_target_layer=true model.layer_norm_targets=true&


PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 optimizer.groups.encoder_with_decay.lr_float=5e-4 optimizer.groups.encoder_no_decay.lr_float=5e-4 optimizer.groups.decoder_with_decay.lr_float=5e-4 optimizer.groups.decoder_no_decay.lr_float=5e-4&
PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 optimizer.groups.encoder_with_decay.lr_float=6e-4 optimizer.groups.encoder_no_decay.lr_float=6e-4 optimizer.groups.decoder_with_decay.lr_float=6e-4 optimizer.groups.decoder_no_decay.lr_float=6e-4&

PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 optimization.max_update=750600 &

// fientunes
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.layer_norm_target_layer:True/model.layer_norm_targets:True/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/optimizer.groups.decoder_no_decay.lr_float:0.0005/optimizer.groups.decoder_with_decay.lr_float:0.0005/optimizer.groups.encoder_no_decay.lr_float:
0.0005/optimizer.groups.encoder_with_decay.lr_float:0.0005/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.ema_decay:0.9992/optimization.max_update:750600/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.ema_anneal_end_step:25000/optimization.max_update:750600/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.ema_anneal_end_step:175000/optimization.max_update:750600/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.ema_anneal_end_step:200000/optimization.max_update:750600/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &

 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.ema_anneal_end_step:125000/optimization.max_update:750600/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.ema_anneal_end_step:50000/optimization.max_update:750600/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.ema_end_decay:0.99999/optimization.max_update:750600/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.ema_decay:0.9994/optimization.max_update:750600/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.ema_anneal_end_step:100000/optimization.max_update:750600/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &

 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.ema_decay:0.999/optimization.max_update:750600/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/optimization.max_update:750600/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &


PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32  hydra.launcher.nodes=4 model.mae_conv_decoder_layers=4&
PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32  hydra.launcher.nodes=4 model.mae_conv_decoder_layers=6&
PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32  hydra.launcher.nodes=4 model.mae_conv_decoder_layers=8&
PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32  hydra.launcher.nodes=4 model.mae_conv_decoder_layers=6 model.mae_conv_decoder_groups=8&
PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32  hydra.launcher.nodes=4 model.mae_conv_decoder_layers=6 model.mae_conv_decoder_groups=4&
PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32  hydra.launcher.nodes=4 model.mask_ratio=0.7&
PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32  hydra.launcher.nodes=4 model.mask_ratio=0.65&

 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.mae_conv_decoder_layers:6/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &

 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.mae_conv_decoder_layers:8/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.mae_conv_decoder_layers:4/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.mae_conv_decoder_groups:8/model.mae_conv_decoder_layers:6/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.mae_conv_decoder_groups:4/model.mae_conv_decoder_layers:6/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &
 PYTHONPATH=. PREFIX=rerun_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/32_cachedv2_d2v/vision/d2v_mae2/large_d2v_mae_imagenet_exp_8/model.average_top_k_layers:22/model.mask_ratio:0.65/checkpoints/checkpoint_last.pt'" +model.add_model_overrides=True optimizer.groups.default.lr_scheduler.min_lr=1e-8 optimization.max_update=125100 &

// even more
PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 optimization.max_update=750600 +model.drop_path_rate=0.05&
PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 optimization.max_update=750600 +model.drop_path_rate=0.1&

PYTHONPATH=.:./examples PREFIX=48_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=48 hydra.launcher.nodes=6 optimization.max_update=750600 &

PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 optimization.max_update=750600 model.ema_decay=0.9999&

PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=32 hydra.launcher.nodes=4 optimization.max_update=1501200 &
PYTHONPATH=.:./examples PREFIX=32_cachedv2_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name large_d2v_mae_imagenet_exp hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_PUSH_blockM/fairseq-py/examples/data2vec\
   task.local_cache_path=/scratch/cache_arbabu/imagenet hydra.launcher.partition=wav2vec model.average_top_k_layers=22 distributed_training.distributed_world_size=48 hydra.launcher.nodes=6 optimization.max_update=1501200 &

