// can we push 83.07 to higher value

PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000&
PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 &
PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=10000&
PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=20000&
PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=30000&
PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000&
PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=50000&

// train longer

PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=16 model.clone_batch=16&

PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=32 model.clone_batch=16&

PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=48 model.clone_batch=16&

PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=64 model.clone_batch=16&

PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=128 model.clone_batch=8&

PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=128 model.clone_batch=16&

PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=256 model.clone_batch=4&

PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=256 model.clone_batch=8&

// half the bsz for all settings and see how curve shifts

PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=25000 model.ema_anneal_end_step=25000 dataset.batch_size=8& 
PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 dataset.batch_size=8& 
PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=100000 model.ema_anneal_end_step=100000 dataset.batch_size=8& 
PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8& 
PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=8 model.clone_batch=32& 

   // ft
failed ones
./wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:256/model.clone_batch:4/hydra_train.lo
./wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.ema_anneal_end_step:50000/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:50000/hydra_train.lo
./wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:48/model.clone_batch:16/hydra_train.lo


 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:8/model.ema_anneal_end_step:50000/optimization.max_update:50000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:8/model.ema_anneal_end_step:25000/optimization.max_update:25000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.ema_anneal_end_step:50000/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:10000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.ema_anneal_end_step:50000/optimization.max_update:50000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.ema_anneal_end_step:50000/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/optimization.max_update:50000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:8/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.ema_anneal_end_step:50000/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:20000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.ema_anneal_end_step:50000/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:30000/checkpoints/checkpoint_last.pt'" &

   

PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  distributed_training.distributed_world_size=1\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=128 model.clone_batch=8 optimization.max_update=2 common.profile=True

//

PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=32&
PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=24&

PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=100000 model.ema_anneal_end_step=100000 dataset.batch_size=8 model.clone_batch=32&  
PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=100000 model.ema_anneal_end_step=100000 dataset.batch_size=8 model.clone_batch=64&  


 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:32/model.clone_batch:16/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:24/model.ema_anneal_end_step:50000/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:32/model.ema_anneal_end_step:50000/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &


//

 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:8/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:8/model.clone_batch:32/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:16/model.clone_batch:16/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:32/model.clone_batch:16/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:24/model.ema_anneal_end_step:50000/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:32/model.ema_anneal_end_step:50000/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:8/model.clone_batch:32/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:8/model.clone_batch:64/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:64/model.clone_batch:16/checkpoints/checkpoint_last.pt'" &

// reduce clone of everything;bsz the same; warmup to slightly lower than total

PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=25000 model.ema_anneal_end_step=25000 optimizer.groups.default.lr_scheduler.warmup_updates=20000 model.clone_batch=8&
PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=8&
PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=100000 model.ema_anneal_end_step=100000 model.clone_batch=8&
PYTHONPATH=.:./examples PREFIX=wallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet model.clone_batch=8&



PYTHONPATH=.:./examples PREFIX=22kwallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   optimization.max_update=100000 model.ema_anneal_end_step=100000 task.data=/datasets01/imagenet-22k/ dataset.train_subset=062717&
PYTHONPATH=.:./examples PREFIX=22kwallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.data=/datasets01/imagenet-22k/ dataset.train_subset=062717&


PYTHONPATH=.:./examples PREFIX=22kwallclock2_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=16 hydra.launcher.nodes=2\
   task.data=/datasets01/imagenet-22k/ dataset.train_subset=062717&

PYTHONPATH=.:./examples PREFIX=22kwallclock2_4node_clip_d2v/vision/d2v_mae2  python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  distributed_training.distributed_world_size=1 \
   optimization.max_update=100000 model.ema_anneal_end_step=100000 task.data=/datasets01/imagenet-22k/ dataset.train_subset=062717


// ft

 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:8/model.ema_anneal_end_step:25000/optimization.max_update:25000/optimizer.groups.default.lr_scheduler.warmup_updates:20000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:8/model.ema_anneal_end_step:50000/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:8/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &

 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:8/checkpoints/checkpoint_last.pt'" &

// try clone_batch=2 and clone_batch=4

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=25000 model.ema_anneal_end_step=25000 optimizer.groups.default.lr_scheduler.warmup_updates=20000 model.clone_batch=4&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=4&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=100000 model.ema_anneal_end_step=100000 model.clone_batch=4&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet model.clone_batch=4&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=25000 model.ema_anneal_end_step=25000 optimizer.groups.default.lr_scheduler.warmup_updates=20000 model.clone_batch=2&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=2&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=100000 model.ema_anneal_end_step=100000 model.clone_batch=2&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet model.clone_batch=2&




PYTHONPATH=.:./examples PREFIX=matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=100000 model.ema_anneal_end_step=100000 model.clone_batch=2,4,8,16,32 dataset.batch_size=2,4,8,16,32&

PYTHONPATH=.:./examples PREFIX=matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=100000 model.ema_anneal_end_step=100000 model.clone_batch=32 dataset.batch_size=32&

PYTHONPATH=.:./examples PREFIX=matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=100000 model.ema_anneal_end_step=100000 model.modalities.image.mask_prob=0.2,0.4,0.6,0.7,0.8,0.9&


//

 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.ema_anneal_end_step:100000/model.modalities.image.mask_prob:0.6/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.ema_anneal_end_step:100000/model.modalities.image.mask_prob:0.7/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.ema_anneal_end_step:100000/model.modalities.image.mask_prob:0.4/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.ema_anneal_end_step:100000/model.modalities.image.mask_prob:0.8/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:4/model.clone_batch:2/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.ema_anneal_end_step:100000/model.modalities.image.mask_prob:0.9/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.ema_anneal_end_step:100000/model.modalities.image.mask_prob:0.2/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:8/model.clone_batch:2/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:2/model.clone_batch:4/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:4/model.clone_batch:4/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:16/model.clone_batch:2/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:2/model.clone_batch:8/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:8/model.clone_batch:4/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:4/model.clone_batch:8/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:16/model.clone_batch:4/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:2/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:32/model.clone_batch:2/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:2/model.clone_batch:16/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:32/model.clone_batch:4/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:8/model.clone_batch:8/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:4/model.clone_batch:16/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:16/model.clone_batch:8/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &

// pending failed ones
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:2/model.clone_batch:2/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:2/model.clone_batch:32/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:4/model.clone_batch:32/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:8/model.clone_batch:16/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:8/model.clone_batch:32/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:16/model.clone_batch:2/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:16/model.clone_batch:16/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:16/model.clone_batch:32/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:32/model.clone_batch:8/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:32/model.clone_batch:16/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &

/// lower clone - full runs


 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:4/model.ema_anneal_end_step:25000/optimization.max_update:25000/optimizer.groups.default.lr_scheduler.warmup_updates:20000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:2/model.ema_anneal_end_step:25000/optimization.max_update:25000/optimizer.groups.default.lr_scheduler.warmup_updates:20000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:4/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:2/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:4/model.ema_anneal_end_step:50000/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:2/model.ema_anneal_end_step:50000/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:2/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallft2_d2v/vision/d2v_mae2/ft/mae_imagenet_clean_2/+task.rebuild_batches:True/common.fp16:True/model.model_path:./fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:2/checkpoints/checkpoint_last.pt./checkpoints/checkpoint_last.pt'" &


//

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=4  model.modalities.image.mask_prob=0.6,0.7&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=4  model.modalities.image.mask_prob=0.6,0.7&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=16  model.modalities.image.mask_prob=0.6,0.7&

 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:4/model.ema_anneal_end_step:50000/model.modalities.image.mask_prob:0.6/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:4/model.ema_anneal_end_step:50000/model.modalities.image.mask_prob:0.7/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:16/model.ema_anneal_end_step:50000/model.modalities.image.mask_prob:0.6/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:16/model.ema_anneal_end_step:50000/model.modalities.image.mask_prob:0.7/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/matrixwallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/dataset.batch_size:32/model.clone_batch:32/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &

// sweep more of 50k to match 83.6

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=16  \
   model.ema_decay=0.999&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=16  \
   model.ema_decay=0.9992&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=16  \
   model.ema_decay=0.9994&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=16  \
   model.ema_end_decay=0.9999&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=16  \
   model.ema_end_decay=0.999999&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=16  \
   model.modalities.image.decoder.decoder_groups=32,8&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=16  \
   model.modalities.image.decoder.decoder_kernel=5,7&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task2 \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=16  \
   model.modalities.image.decoder.decoder_layers=3,4,8&


 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:16/model.ema_anneal_end_step:50000/model.modalities.image.decoder.decoder_layers:3/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:16/model.ema_anneal_end_step:50000/model.ema_end_decay:0.999999/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:16/model.ema_anneal_end_step:50000/model.modalities.image.decoder.decoder_layers:4/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:16/model.ema_anneal_end_step:50000/model.ema_end_decay:0.9999/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:16/model.ema_anneal_end_step:50000/model.ema_decay:0.9992/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:16/model.ema_anneal_end_step:50000/model.modalities.image.decoder.decoder_groups:32/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:16/model.ema_anneal_end_step:50000/model.ema_decay:0.999/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:16/model.ema_anneal_end_step:50000/model.modalities.image.decoder.decoder_kernel:5/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:16/model.ema_anneal_end_step:50000/model.ema_decay:0.9994/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:16/model.ema_anneal_end_step:50000/model.modalities.image.decoder.decoder_layers:8/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:16/model.ema_anneal_end_step:50000/model.modalities.image.decoder.decoder_groups:8/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task2_4/model.clone_batch:16/model.ema_anneal_end_step:50000/model.modalities.image.decoder.decoder_kernel:7/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &

// final final ablations

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=25000 model.ema_anneal_end_step=25000 optimizer.groups.default.lr_scheduler.warmup_updates=20000 model.clone_batch=8  &
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=25000 model.ema_anneal_end_step=25000 optimizer.groups.default.lr_scheduler.warmup_updates=20000 model.clone_batch=8  model.ema_decay=0.999&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000 model.ema_anneal_end_step=50000 optimizer.groups.default.lr_scheduler.warmup_updates=40000 model.clone_batch=16  \
   model.ema_decay=0.999&


PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=100000 model.ema_anneal_end_step=100000 model.clone_batch=8  &

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=100000 model.ema_anneal_end_step=100000 model.clone_batch=8  model.ema_decay=0.999&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=200000 model.clone_batch=8  &

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=200000 model.clone_batch=8  model.ema_decay=0.999&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet model.ema_decay=0.999&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet &

// sweep 32 bsz

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=32 &

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=32 model.ema_anneal_end_step=70000&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=32 model.ema_anneal_end_step=150000&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=32 model.ema_decay=0.999&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=32 model.ema_decay=0.9992&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet dataset.batch_size=32 model.ema_decay=0.994&






// huge experiments


PYTHONPATH=.:./examples python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec   distributed_training.distributed_world_size=1

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name large_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=100000&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name large_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet &

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet &

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.lr=[0.0002]&


PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name large_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=200000&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name large_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=50000&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name large_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.max_update=25000&



// vit-b decoder sweep


PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet   model.modalities.image.decoder.decoder_layers=1&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet   model.modalities.image.decoder.decoder_layers=8&


PYTHONPATH=.:./examples PREFIX=testsetesttest  python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec   distributed_training.distributed_world_size=1\
   task.local_cache_path=/scratch/cache_arbabu/imagenet   model.modalities.image.decoder.decoder_layers=1 +model.modalities.image.transformer_decoder=True

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet   model.modalities.image.decoder.decoder_layers=1 +model.modalities.image.transformer_decoder=True&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet   model.modalities.image.decoder.decoder_layers=8 +model.modalities.image.transformer_decoder=True&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name base_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet   +model.modalities.image.transformer_decoder=True&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_mae_imagenet \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet   +model.decoder_depth=1&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_mae_imagenet \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet   +model.decoder_depth=8&


 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task_4/model.clone_batch:8/model.ema_anneal_end_step:25000/optimization.max_update:25000/optimizer.groups.default.lr_scheduler.warmup_updates:20000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task_4/model.clone_batch:16/model.ema_anneal_end_step:50000/model.ema_decay:0.999/optimization.max_update:50000/optimizer.groups.default.lr_scheduler.warmup_updates:40000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task_4/dataset.batch_size:32/model.ema_anneal_end_step:70000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task_4/dataset.batch_size:32/model.ema_decay:0.994/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task_4/dataset.batch_size:32/model.ema_decay:0.999/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task_4/model.clone_batch:8/model.ema_anneal_end_step:25000/model.ema_decay:0.999/optimization.max_update:25000/optimizer.groups.default.lr_scheduler.warmup_updates:20000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task_4/model.clone_batch:8/model.ema_anneal_end_step:100000/model.ema_decay:0.999/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task_4/model.clone_batch:8/model.ema_anneal_end_step:100000/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &

   

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2  python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_mae_imagenet \
   common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  distributed_training.distributed_world_size=1\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet   +model.decoder_depth=8

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_mae_imagenet \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet   +model.decoder_depth=1 dataset.batch_size=128&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_mae_imagenet \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet   +model.decoder_depth=8 dataset.batch_size=128&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/pretraining --config-name base_mae_imagenet \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   +task.local_cache_path=/scratch/cache_arbabu/imagenet   +model.decoder_depth=6 dataset.batch_size=128&


//

 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task_4/dataset.batch_size:32/model.ema_anneal_end_step:150000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task_4/dataset.batch_size:32/model.ema_decay:0.9992/checkpoints/checkpoint_last.pt'" &
   
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task_4/model.clone_batch:8/optimization.max_update:200000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/base_images_only_task_4/checkpoints/checkpoint_last.pt'" &
   

// huge runs

 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_huge_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/huge_images_only_task_4/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_huge_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/huge_images_only_task_4/optimization.lr:0.0002/checkpoints/checkpoint_last.pt'" &
   

// large finetuning

 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_large_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/large_images_only_task_4/optimization.max_update:100000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_large_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/large_images_only_task_4/optimization.max_update:25000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_large_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/large_images_only_task_4/optimization.max_update:50000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_large_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/large_images_only_task_4/optimization.max_update:200000/checkpoints/checkpoint_last.pt'" &
 PYTHONPATH=. PREFIX=wallft2_d2v/vision/d2v_mae2/ft nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/vision/finetuning --config-name mae_imagenet_large_clean hydra/launcher=submitit_slurm +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] \
    +task.local_cache_path=/scratch/cache_arbabu/imagenet common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  common.fp16=true \
    distributed_training.find_unused_parameters=true +task.rebuild_batches=true \
    "model.model_path='/fsx-wav2vec/arbabu/d2v2_logs/wallclock2fx_4node_clip_d2v/vision/d2v_mae2/large_images_only_task_4/checkpoints/checkpoint_last.pt'" &

   

// huge sweeps

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.lr=[0.0001]&

PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.lr=[0.0002] model.average_top_k_layers=32&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.lr=[0.0002] model.average_top_k_layers=30&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.lr=[0.0002] model.average_top_k_layers=28&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.lr=[0.0002] model.average_top_k_layers=26&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.lr=[0.0002] model.average_top_k_layers=22&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.lr=[0.0002] model.average_top_k_layers=18&


PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.lr=[0.0002] model.ema_anneal_end_step=70000&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.lr=[0.0002] model.ema_anneal_end_step=100000&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.lr=[0.0002] model.ema_anneal_end_step=125000&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.lr=[0.0002] model.ema_anneal_end_step=175000&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.lr=[0.0002] model.ema_anneal_end_step=200000&


PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.lr=[0.0002] model.ema_end_decay=0.99998&
PYTHONPATH=.:./examples PREFIX=wallclock2fx_4node_clip_d2v/vision/d2v_mae2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/multi --config-name huge_images_only_task \
   hydra/launcher=submitit_slurm \
   +run_config=slurm_2_aws_arbabu distributed_training.distributed_port=$[${RANDOM}%30000+30000] common.user_dir=/data/home/arbabu/d2v_working_rc/fairseq-py/examples/data2vec  hydra.launcher.partition=wav2vec distributed_training.distributed_world_size=32 hydra.launcher.nodes=4\
   task.local_cache_path=/scratch/cache_arbabu/imagenet optimization.lr=[0.0002] model.ema_end_decay=1&
