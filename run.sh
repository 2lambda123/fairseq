# exps=(
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.average_top_k_layers_12/model.ema_anneal_end_step_30000/optimization.max_update_100000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.average_top_k_layers_6/model.ema_anneal_end_step_30000/optimization.max_update_100000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.average_top_k_layers_8/model.ema_anneal_end_step_30000/optimization.max_update_100000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/model.head_layers_1/optimization.max_update_100000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/model.head_layers_3/optimization.max_update_100000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/model.mae_decoder_dim_1024/optimization.max_update_100000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/model.mae_decoder_dim_384/optimization.max_update_100000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/model.mae_decoder_heads_12/optimization.max_update_100000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/model.mae_decoder_heads_4/optimization.max_update_100000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/model.mae_decoder_layers_2/optimization.max_update_100000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/model.mae_decoder_layers_3/optimization.max_update_100000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/model.mae_decoder_layers_4/optimization.max_update_100000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/model.mae_encoder_decoder_attn_False/model.mae_no_self_attn_False/optimization.max_update_100000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/optimization.max_update_100000/optimizer.groups.decoder.lr_float_1e-06/optimizer.groups.encoder.lr_float_0.0003/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/optimization.max_update_100000/optimizer.groups.decoder.lr_float_3e-05/optimizer.groups.encoder.lr_float_0.0003/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/optimization.max_update_100000/optimizer.groups.decoder.lr_float_3e-07/optimizer.groups.encoder.lr_float_0.0003/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/optimization.max_update_100000/optimizer.groups.decoder.lr_float_5e-06/optimizer.groups.encoder.lr_float_0.0003/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/optimization.max_update_100000/optimizer.groups.decoder.lr_float_5e-06/optimizer.groups.encoder.lr_float_0.0005/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
#   # "/fsx-wav2vec/wnhsu/d2v2/swp2/base_dual_mae_aws_8/dataset.batch_size_16/model.ema_anneal_end_step_30000/optimization.max_update_100000/optimizer.groups.decoder.lr_float_7e-06/optimizer.groups.encoder.lr_float_0.0007/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000/optimizer.groups.encoder.lr_scheduler.warmup_updates_1000"
# )
# 
# for exp in ${exps[@]}; do
#   examples/data2vec/scripts/text/finetune_all_fair_nodep_aws_lr.sh $exp
# done

# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_ema nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.ema_anneal_end_step=10000 model.ema_end_decay=0.9999 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_ema nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.ema_anneal_end_step=20000 model.ema_end_decay=0.9999 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_ema nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.ema_anneal_end_step=50000 model.ema_end_decay=0.9999 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_ema nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.ema_anneal_end_step=100000 model.ema_end_decay=0.9999 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_ema nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.ema_anneal_end_step=10000 model.ema_end_decay=1 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_ema nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.ema_anneal_end_step=20000 model.ema_end_decay=1 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_ema nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.ema_anneal_end_step=50000 model.ema_end_decay=1 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_ema nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.ema_anneal_end_step=100000 model.ema_end_decay=1 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_pos nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.transformer.encoder.learned_pos=false &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_pos nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 +model.mae_decoder_learned_pos=false &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_pos nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.transformer.encoder.learned_pos=false +model.mae_decoder_learned_pos=false &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_norm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.batch_norm_target_layer=false +model.instance_norm_targets=true +model.layer_norm_targets=false &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_norm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.layer_norm_target_layer=false +model.instance_norm_target_layer=true +model.batch_norm_target_layer=false +model.instance_norm_targets=true +model.layer_norm_targets=false &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_norm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.layer_norm_target_layer=false +model.instance_norm_target_layer=false +model.batch_norm_target_layer=true +model.instance_norm_targets=true +model.layer_norm_targets=false &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_norm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.batch_norm_target_layer=false +model.instance_norm_targets=false +model.layer_norm_targets=true &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_norm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.layer_norm_target_layer=false +model.instance_norm_target_layer=true +model.batch_norm_target_layer=false +model.instance_norm_targets=false +model.layer_norm_targets=true &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_norm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 model.layer_norm_target_layer=false +model.instance_norm_target_layer=false +model.batch_norm_target_layer=true +model.instance_norm_targets=false +model.layer_norm_targets=true &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_lrwarm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 optimizer.groups.encoder.lr_scheduler.warmup_updates=2000 optimizer.groups.decoder.lr_scheduler.warmup_updates=2000 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_lrwarm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 optimizer.groups.encoder.lr_scheduler.warmup_updates=5000 optimizer.groups.decoder.lr_scheduler.warmup_updates=5000 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_lrwarm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 optimizer.groups.encoder.lr_scheduler.warmup_updates=10000 optimizer.groups.decoder.lr_scheduler.warmup_updates=10000 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_lrwarm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 optimizer.groups.encoder.lr_scheduler.warmup_updates=20000 optimizer.groups.decoder.lr_scheduler.warmup_updates=20000 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp3_lrwarm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 optimizer.groups.encoder.lr_scheduler.warmup_updates=30000 optimizer.groups.decoder.lr_scheduler.warmup_updates=30000 &


# PYTHONPATH=$(pwd) PREFIX=d2v2/swp4_mask nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 task.mask_multiple_length=2 task.mask_prob=0.08 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp4_mask nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 task.mask_multiple_length=4 task.mask_prob=0.08 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp4_mask nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 task.mask_multiple_length=6 task.mask_prob=0.08 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp4_mask nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 task.mask_multiple_length=1 task.mask_prob=0.04 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp4_mask nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 task.mask_multiple_length=1 task.mask_prob=0.16 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp4_mask nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 task.mask_multiple_length=1 task.mask_prob=0.32 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp4_mask nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 task.mask_multiple_length=2 task.mask_prob=0.04 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp4_mask nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 task.mask_multiple_length=2 task.mask_prob=0.16 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp4_mask nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 task.mask_multiple_length=2 task.mask_prob=0.32 &

# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] dataset.batch_size=16 lr_scheduler.warmup_updates=1000 optimization.max_update=100000 model.ema_anneal_end_step=30000 &

# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline3 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline3 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=true &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline3 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=false +model.instance_norm_target_layer=true +model.layer_norm_targets=true &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline3 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false +model.remask=True &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline3 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false +model.remask=True common.seed=1,2,3,4,5 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline4_remask_dbg nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false +model.remask=False +model.copy_mask=False &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline4_remask_dbg nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false +model.remask=True +model.copy_mask=False &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline4_remask_dbg nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false +model.remask=True +model.copy_mask=True &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline4_remask_dbg nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false +model.remask=True common.fp16=False &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline4_remask_dbg nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false +model.remask=True optimization.clip_norm=0.5 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline4_remask_dbg nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false +model.remask=True +model.min_masks=0 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline4_remask_dbg nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false +model.remask=False optimization.clip_norm=0.5 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline4_remask_dbg nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false +model.remask=False 'optimization.lr=[0.0002]' &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline4_remask_dbg nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false +model.remask=True 'optimization.lr=[0.0002]' &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline4_remask_dbg/2n nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_2_aws  distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false +model.remask=True 'optimization.lr=[0.0002]' &

# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline4_remask_dbg2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false +model.remask=True 'optimization.lr=[0.0003]' +model.log_mask_stat=True &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline4_remask_dbg2 nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false +model.remask=True 'optimization.lr=[0.0002]' +model.log_mask_stat=True &

# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_norm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=false &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_norm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=true +model.instance_norm_target_layer=false +model.layer_norm_targets=true  &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_norm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] model.layer_norm_target_layer=false +model.instance_norm_target_layer=true +model.layer_norm_targets=true  &

# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_lrwarm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] optimizer.groups.encoder.lr_scheduler.warmup_updates=10000 optimizer.groups.decoder.lr_scheduler.warmup_updates=10000 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_lrwarm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] optimizer.groups.encoder.lr_scheduler.warmup_updates=20000 optimizer.groups.decoder.lr_scheduler.warmup_updates=20000 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_lrwarm nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] optimizer.groups.encoder.lr_scheduler.warmup_updates=30000 optimizer.groups.decoder.lr_scheduler.warmup_updates=30000 &

# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_conv nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=1 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_conv nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=2 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_conv nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_conv nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=4 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_conv nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=5 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 &
# 
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_conv_lr nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=1e-4 optimizer.groups.decoder.lr_float=1e-4 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_conv_lr nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=5e-4 optimizer.groups.decoder.lr_float=5e-4 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_conv_lr nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=7e-4 optimizer.groups.decoder.lr_float=7e-4 &
# 
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_conv_kernel nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_kernel=3 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_conv_kernel nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_kernel=7 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_conv_kernel nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_kernel=9 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_conv_kernel nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_kernel=11 &
# 
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_conv_group nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_groups=2 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_conv_group nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_groups=4 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp5_conv_group nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8 distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_groups=8 &

# PYTHONPATH=$(pwd) PREFIX=d2v2/swp6_conv nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_groups=4 +model.mae_conv_decoder_kernel=3 dataset.batch_size=16 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp6_conv nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=2e-4 optimizer.groups.decoder.lr_float=2e-4 +model.mae_conv_decoder_groups=4 +model.mae_conv_decoder_kernel=3 dataset.batch_size=16 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp6_conv nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=4e-4 optimizer.groups.decoder.lr_float=4e-4 +model.mae_conv_decoder_groups=4 +model.mae_conv_decoder_kernel=3 dataset.batch_size=16 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp6_conv nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_groups=1 +model.mae_conv_decoder_kernel=3 dataset.batch_size=16 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp6_conv nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_groups=2 +model.mae_conv_decoder_kernel=3 dataset.batch_size=16 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp6_conv nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_groups=4 +model.mae_conv_decoder_kernel=3 dataset.batch_size=16 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp6_conv nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_groups=4 +model.mae_conv_decoder_kernel=3 dataset.batch_size=32 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp6_conv_2n nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_2_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_groups=4 +model.mae_conv_decoder_kernel=3 dataset.batch_size=16 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp6_conv_step nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_groups=4 +model.mae_conv_decoder_kernel=3 dataset.batch_size=16 optimization.max_update=400000 optimizer.groups.decoder.lr_scheduler.warmup_updates=1000 optimizer.groups.encoder.lr_scheduler.warmup_updates=1000 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp6_conv_step nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_groups=4 +model.mae_conv_decoder_kernel=3 dataset.batch_size=16 optimization.max_update=400000 optimizer.groups.decoder.lr_scheduler.warmup_updates=2000 optimizer.groups.encoder.lr_scheduler.warmup_updates=2000 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp6_conv_step nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_groups=4 +model.mae_conv_decoder_kernel=3 dataset.batch_size=16 optimization.max_update=400000 optimizer.groups.decoder.lr_scheduler.warmup_updates=4000 optimizer.groups.encoder.lr_scheduler.warmup_updates=4000 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/swp6_conv_step nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_port=$[${RANDOM}%30000+30000] +model.mae_conv_decoder_layers=3 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-4 +model.mae_conv_decoder_groups=4 +model.mae_conv_decoder_kernel=3 dataset.batch_size=16 optimization.max_update=200000 optimizer.groups.decoder.lr_scheduler.warmup_updates=2000 optimizer.groups.encoder.lr_scheduler.warmup_updates=2000 &

# ====================

# d2v1_cmd="nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8"
# d2v2_cmd="nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws distributed_training.distributed_world_size=8"
# port="distributed_training.distributed_port=$[${RANDOM}%30000+30000]"

# PREFIX=d2v2/baseline4 PYTHONPATH=$(pwd) $d2v1_cmd $port +model.remask=False common.seed=2,3,4 'optimization.lr=[0.0003],[0.0004],[0.0005]' &
# PREFIX=d2v2/swp7_alibi PYTHONPATH=$(pwd) $d2v2_cmd $port +model.mae_conv_decoder_layers=3 +model.mae_conv_decoder_groups=1 +model.mae_conv_decoder_kernel=3 +model.use_alibi_encoder=False &
# PREFIX=d2v2/swp7_alibi PYTHONPATH=$(pwd) $d2v2_cmd $port +model.mae_conv_decoder_layers=3 +model.mae_conv_decoder_groups=1 +model.mae_conv_decoder_kernel=3 +model.use_alibi_encoder=True &
# PREFIX=d2v2/swp7_convres PYTHONPATH=$(pwd) $d2v2_cmd $port +model.mae_conv_decoder_layers=1 +model.mae_conv_decoder_groups=1 +model.mae_conv_decoder_kernel=3 +model.mae_conv_decoder_residual=True &
# PREFIX=d2v2/swp7_convres PYTHONPATH=$(pwd) $d2v2_cmd $port +model.mae_conv_decoder_layers=2 +model.mae_conv_decoder_groups=1 +model.mae_conv_decoder_kernel=3 +model.mae_conv_decoder_residual=True &
# PREFIX=d2v2/swp7_convres PYTHONPATH=$(pwd) $d2v2_cmd $port +model.mae_conv_decoder_layers=3 +model.mae_conv_decoder_groups=1 +model.mae_conv_decoder_kernel=3 +model.mae_conv_decoder_residual=True &
# PREFIX=d2v2/swp7_convres PYTHONPATH=$(pwd) $d2v2_cmd $port +model.mae_conv_decoder_layers=4 +model.mae_conv_decoder_groups=1 +model.mae_conv_decoder_kernel=3 +model.mae_conv_decoder_residual=True &
# PREFIX=d2v2/swp7_convres PYTHONPATH=$(pwd) $d2v2_cmd $port +model.mae_conv_decoder_layers=5 +model.mae_conv_decoder_groups=1 +model.mae_conv_decoder_kernel=3 +model.mae_conv_decoder_residual=True &
# PREFIX=d2v2/swp7_clone PYTHONPATH=$(pwd) $d2v2_cmd $port +model.mae_conv_decoder_layers=3 +model.mae_conv_decoder_groups=1 +model.mae_conv_decoder_kernel=3 dataset.batch_size=16 +model.clone_batch=1 &
# PREFIX=d2v2/swp7_clone PYTHONPATH=$(pwd) $d2v2_cmd $port +model.mae_conv_decoder_layers=3 +model.mae_conv_decoder_groups=1 +model.mae_conv_decoder_kernel=3 dataset.batch_size=8 +model.clone_batch=2 &
# PREFIX=d2v2/swp7_clone PYTHONPATH=$(pwd) $d2v2_cmd $port +model.mae_conv_decoder_layers=3 +model.mae_conv_decoder_groups=1 +model.mae_conv_decoder_kernel=3 dataset.batch_size=8 +model.clone_batch=4 &
# PREFIX=d2v2/swp7_clone PYTHONPATH=$(pwd) $d2v2_cmd $port +model.mae_conv_decoder_layers=3 +model.mae_conv_decoder_groups=1 +model.mae_conv_decoder_kernel=3 dataset.batch_size=4 +model.clone_batch=4 &
# PREFIX=d2v2/swp7_clone PYTHONPATH=$(pwd) $d2v2_cmd $port +model.mae_conv_decoder_layers=3 +model.mae_conv_decoder_groups=1 +model.mae_conv_decoder_kernel=3 dataset.batch_size=4 +model.clone_batch=8 &
# PREFIX=d2v2/swp7_clone PYTHONPATH=$(pwd) $d2v2_cmd $port +model.mae_conv_decoder_layers=3 +model.mae_conv_decoder_groups=1 +model.mae_conv_decoder_kernel=3 dataset.batch_size=4 +model.clone_batch=16 &


# ==================== d2v2 full sweep round 1


# d2v2_full_cmd="nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_conv_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_2_aws"
# port="distributed_training.distributed_port=$[${RANDOM}%30000+30000]"

# # baseline
# PREFIX=d2v2/full_swp1_baseline PYTHONPATH=$(pwd) $d2v2_full_cmd $port &

# # decoder LR (default=3e-4)
# PREFIX=d2v2/full_swp1_dec_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.decoder.lr_float=7e-5 &
# PREFIX=d2v2/full_swp1_dec_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.decoder.lr_float=1e-4 &
# PREFIX=d2v2/full_swp1_dec_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.decoder.lr_float=5e-4 &
# PREFIX=d2v2/full_swp1_dec_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.decoder.lr_float=1e-3 &
# 
# # encoder LR (default=3e-4)
# PREFIX=d2v2/full_swp1_enc_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.encoder.lr_float=7e-5 &
# PREFIX=d2v2/full_swp1_enc_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.encoder.lr_float=1e-4 &
# PREFIX=d2v2/full_swp1_enc_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.encoder.lr_float=5e-4 &
# PREFIX=d2v2/full_swp1_enc_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.encoder.lr_float=1e-3 &
# 
# # LR warmup (default=4000)
# PREFIX=d2v2/full_swp1_lrwarm PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.encoder.lr_scheduler.warmup_updates=1000  optimizer.groups.decoder.lr_scheduler.warmup_updates=1000 &
# PREFIX=d2v2/full_swp1_lrwarm PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.encoder.lr_scheduler.warmup_updates=8000  optimizer.groups.decoder.lr_scheduler.warmup_updates=8000 &
# PREFIX=d2v2/full_swp1_lrwarm PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.encoder.lr_scheduler.warmup_updates=16000  optimizer.groups.decoder.lr_scheduler.warmup_updates=16000 &
# 
# # mask length (default=1)
# PREFIX=d2v2/full_swp1_maskl PYTHONPATH=$(pwd) $d2v2_full_cmd $port task.mask_multiple_length=2 &
# PREFIX=d2v2/full_swp1_maskl PYTHONPATH=$(pwd) $d2v2_full_cmd $port task.mask_multiple_length=4 &
# PREFIX=d2v2/full_swp1_maskl PYTHONPATH=$(pwd) $d2v2_full_cmd $port task.mask_multiple_length=6 &
# PREFIX=d2v2/full_swp1_maskl PYTHONPATH=$(pwd) $d2v2_full_cmd $port task.mask_multiple_length=8 &
# 
# # mask prob (default=0.08)
# PREFIX=d2v2/full_swp1_maskp PYTHONPATH=$(pwd) $d2v2_full_cmd $port task.mask_prob=0.04 &
# PREFIX=d2v2/full_swp1_maskp PYTHONPATH=$(pwd) $d2v2_full_cmd $port task.mask_prob=0.12 &
# PREFIX=d2v2/full_swp1_maskp PYTHONPATH=$(pwd) $d2v2_full_cmd $port task.mask_prob=0.16 &
# PREFIX=d2v2/full_swp1_maskp PYTHONPATH=$(pwd) $d2v2_full_cmd $port task.mask_prob=0.20 &
# PREFIX=d2v2/full_swp1_maskp PYTHONPATH=$(pwd) $d2v2_full_cmd $port task.mask_prob=0.24 &
# 
# # target num layers (default=10)
# PREFIX=d2v2/full_swp1_tgtlyr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.average_top_k_layers=12 &
# PREFIX=d2v2/full_swp1_tgtlyr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.average_top_k_layers=11 &
# PREFIX=d2v2/full_swp1_tgtlyr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.average_top_k_layers=9 &
# PREFIX=d2v2/full_swp1_tgtlyr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.average_top_k_layers=6 &
# PREFIX=d2v2/full_swp1_tgtlyr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.average_top_k_layers=3 &
# 
# # target normalization (default: layer_norm_target_layer=true)
# PREFIX=d2v2/full_swp1_tgtnorm PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.layer_norm_target_layer=true model.instance_norm_target_layer=false model.batch_norm_target_layer=false model.instance_norm_targets=false model.layer_norm_targets=false &
# PREFIX=d2v2/full_swp1_tgtnorm PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.layer_norm_target_layer=false model.instance_norm_target_layer=true model.batch_norm_target_layer=false model.instance_norm_targets=false model.layer_norm_targets=false &
# PREFIX=d2v2/full_swp1_tgtnorm PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.layer_norm_target_layer=false model.instance_norm_target_layer=false model.batch_norm_target_layer=true model.instance_norm_targets=false model.layer_norm_targets=false &
# PREFIX=d2v2/full_swp1_tgtnorm PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.layer_norm_target_layer=true model.instance_norm_target_layer=false model.batch_norm_target_layer=false model.instance_norm_targets=true model.layer_norm_targets=false &
# PREFIX=d2v2/full_swp1_tgtnorm PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.layer_norm_target_layer=false model.instance_norm_target_layer=true model.batch_norm_target_layer=false model.instance_norm_targets=true model.layer_norm_targets=false &
# PREFIX=d2v2/full_swp1_tgtnorm PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.layer_norm_target_layer=false model.instance_norm_target_layer=false model.batch_norm_target_layer=true model.instance_norm_targets=true model.layer_norm_targets=false &
# PREFIX=d2v2/full_swp1_tgtnorm PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.layer_norm_target_layer=true model.instance_norm_target_layer=false model.batch_norm_target_layer=false model.instance_norm_targets=false model.layer_norm_targets=true &
# PREFIX=d2v2/full_swp1_tgtnorm PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.layer_norm_target_layer=false model.instance_norm_target_layer=true model.batch_norm_target_layer=false model.instance_norm_targets=false model.layer_norm_targets=true &
# PREFIX=d2v2/full_swp1_tgtnorm PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.layer_norm_target_layer=false model.instance_norm_target_layer=false model.batch_norm_target_layer=true model.instance_norm_targets=false model.layer_norm_targets=true &
# 
# # decoder layer (default=3)
# PREFIX=d2v2/full_swp1_declyr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_layers=1 &
# PREFIX=d2v2/full_swp1_declyr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_layers=2 &
# PREFIX=d2v2/full_swp1_declyr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_layers=4 &
# PREFIX=d2v2/full_swp1_declyr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_layers=5 &
# 
# # decoder group (default=4)
# PREFIX=d2v2/full_swp1_decgrp PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_groups=1 &
# PREFIX=d2v2/full_swp1_decgrp PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_groups=2 &
# PREFIX=d2v2/full_swp1_decgrp PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_groups=8 &
# PREFIX=d2v2/full_swp1_decgrp PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_groups=16 &
# 
# # decoder kernel
# PREFIX=d2v2/full_swp1_kernel PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_kernel=1 &
# PREFIX=d2v2/full_swp1_kernel PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_kernel=3 &
# PREFIX=d2v2/full_swp1_kernel PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_kernel=7 &
# PREFIX=d2v2/full_swp1_kernel PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_kernel=9 &
# PREFIX=d2v2/full_swp1_kernel PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_kernel=11 &
# 
# # decoder dropout (0.0)
# PREFIX=d2v2/full_swp1_decdrop PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_decoder_dropout=0.05 &
# PREFIX=d2v2/full_swp1_decdrop PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_decoder_dropout=0.07 &
# PREFIX=d2v2/full_swp1_decdrop PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_decoder_dropout=0.10 &
# PREFIX=d2v2/full_swp1_decdrop PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_decoder_dropout=0.15 &
# 
# # decoder residual
# PREFIX=d2v2/full_swp1_decres PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_residual=True &
# 
# # decoder alibi
# PREFIX=d2v2/full_swp1_alibi PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.use_alibi_encoder=True &
# 
# # ema end step (30000)
# PREFIX=d2v2/full_swp1_ema_schd PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_anneal_end_step=10000 &
# PREFIX=d2v2/full_swp1_ema_schd PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_anneal_end_step=50000 &
# PREFIX=d2v2/full_swp1_ema_schd PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_anneal_end_step=75000 &
# PREFIX=d2v2/full_swp1_ema_schd PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_anneal_end_step=100000 &
# PREFIX=d2v2/full_swp1_ema_schd PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_anneal_end_step=400000 &
# 
# # ema value
# PREFIX=d2v2/full_swp1_ema_rate PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_decay=0.999 model.ema_end_decay=0.9999 &
# PREFIX=d2v2/full_swp1_ema_rate PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_decay=0.999 model.ema_end_decay=0.99995 &
# PREFIX=d2v2/full_swp1_ema_rate PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_decay=0.999 model.ema_end_decay=0.99999 &
# PREFIX=d2v2/full_swp1_ema_rate PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_decay=0.9995 model.ema_end_decay=0.9999 &
# PREFIX=d2v2/full_swp1_ema_rate PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_decay=0.9995 model.ema_end_decay=0.99995 &
# PREFIX=d2v2/full_swp1_ema_rate PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_decay=0.9995 model.ema_end_decay=0.99999 &
# PREFIX=d2v2/full_swp1_ema_rate PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_decay=0.9999 model.ema_end_decay=0.99992 &
# PREFIX=d2v2/full_swp1_ema_rate PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_decay=0.9999 model.ema_end_decay=0.99995 &
# PREFIX=d2v2/full_swp1_ema_rate PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_decay=0.9999 model.ema_end_decay=0.99999 &
# 
# # clone batch
# PREFIX=d2v2/full_swp1_clone PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=1 dataset.batch_size=16 &
# PREFIX=d2v2/full_swp1_clone PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=2 dataset.batch_size=16 &
# PREFIX=d2v2/full_swp1_clone PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=4 dataset.batch_size=16 &
# PREFIX=d2v2/full_swp1_clone PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=8 dataset.batch_size=8 &
# PREFIX=d2v2/full_swp1_clone PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=8 dataset.batch_size=4 &


# ==================== check remasking versus masked_token_dataset

# d2v_cmd="nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh +run_config=slurm_1_aws"
# port="distributed_training.distributed_port=$[${RANDOM}%30000+30000]"
# 
# 
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline5_remask_dbg $d2v_cmd $port 'optimization.lr=[0.0003]' +task.include_index=True +task.skip_masking=True +model.min_masks=0 common.seed=3,4,5 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/baseline5_remask_dbg $d2v_cmd $port 'optimization.lr=[0.0003]' +model.min_masks=0 common.seed=3,4,5 &
#


# ==================== debug masking
d2v_cmd="nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_local_lr.sh +run_config=slurm_1_aws"
port="distributed_training.distributed_port=$[${RANDOM}%30000+30000]"

# PYTHONPATH=$(pwd) PREFIX=d2v2/dbg_mask_run2 $d2v_cmd $port +model.log_mask_stat=True common.log_interval=1 dataset.num_workers=0 +task.include_index=True +task.subsample_train=0.001 common.seed=1 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/dbg_mask_run2 $d2v_cmd $port +model.log_mask_stat=True common.log_interval=1 dataset.num_workers=0 +task.include_index=True +task.subsample_train=0.001 common.seed=1 +task.skip_masking=True +model.remask_ver=1 +model.idc_select_ver=1 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/dbg_mask_run2 $d2v_cmd $port +model.log_mask_stat=True common.log_interval=1 dataset.num_workers=0 +task.include_index=True +task.subsample_train=0.001 common.seed=2 +task.skip_masking=True +model.remask_ver=1 +model.idc_select_ver=1 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/dbg_mask_run2 $d2v_cmd $port +model.log_mask_stat=True common.log_interval=1 dataset.num_workers=0 +task.include_index=True +task.subsample_train=0.001 common.seed=1 +task.skip_masking=True +model.remask_ver=1 +model.idc_select_ver=2 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/dbg_mask_run2 $d2v_cmd $port +model.log_mask_stat=True common.log_interval=1 dataset.num_workers=0 +task.include_index=True +task.subsample_train=0.001 common.seed=2 +task.skip_masking=True +model.remask_ver=1 +model.idc_select_ver=2 &

# PYTHONPATH=$(pwd) PREFIX=d2v2/dbg_mask_run3 $d2v_cmd $port +model.log_mask_stat=True common.log_interval=1 dataset.num_workers=0 +task.include_index=True common.seed=1 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/dbg_mask_run3 $d2v_cmd $port +model.log_mask_stat=True common.log_interval=1 dataset.num_workers=0 +task.include_index=True common.seed=1 +task.skip_masking=True +model.remask_ver=1 +model.idc_select_ver=1 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/dbg_mask_run3 $d2v_cmd $port +model.log_mask_stat=True common.log_interval=1 dataset.num_workers=0 +task.include_index=True common.seed=2 +task.skip_masking=True +model.remask_ver=1 +model.idc_select_ver=1 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/dbg_mask_run3 $d2v_cmd $port +model.log_mask_stat=True common.log_interval=1 dataset.num_workers=0 +task.include_index=True common.seed=1 +task.skip_masking=True +model.remask_ver=1 +model.idc_select_ver=2 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/dbg_mask_run3 $d2v_cmd $port +model.log_mask_stat=True common.log_interval=1 dataset.num_workers=0 +task.include_index=True common.seed=2 +task.skip_masking=True +model.remask_ver=1 +model.idc_select_ver=2 &

# PYTHONPATH=$(pwd) PREFIX=d2v2/dbg_mask_run4 $d2v_cmd $port +model.log_mask_stat=True common.log_interval=1 dataset.num_workers=0 +task.include_index=True common.seed=1 +task.skip_masking=True +model.remask_ver=1 +model.idc_select_ver=1 +self.cfg.num_mask_ver=1 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/dbg_mask_run4 $d2v_cmd $port +model.log_mask_stat=True common.log_interval=1 dataset.num_workers=0 +task.include_index=True common.seed=2 +task.skip_masking=True +model.remask_ver=1 +model.idc_select_ver=1 +self.cfg.num_mask_ver=2 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/dbg_mask_run4 $d2v_cmd $port +model.log_mask_stat=True common.log_interval=1 dataset.num_workers=0 +task.include_index=True common.seed=2 +task.skip_masking=False +model.remask_ver=1 +model.idc_select_ver=1 +self.cfg.num_mask_ver=2 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/dbg_mask_run4 $d2v_cmd $port +model.log_mask_stat=True common.log_interval=1 dataset.num_workers=0 +task.include_index=True common.seed=1 +task.skip_masking=True +model.remask_ver=3 &
# PYTHONPATH=$(pwd) PREFIX=d2v2/dbg_mask_run4_seed-mask-ver-3 $d2v_cmd $port +model.log_mask_stat=True common.log_interval=1 dataset.num_workers=0 +task.include_index=True common.seed=1 +task.skip_masking=True +model.remask_ver=3 &

# # ==================== d2v2 full sweep round 2
# 
# d2v2_full_cmd="nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_conv_v2_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_local_lr.sh +run_config=slurm_2_aws"
# port="distributed_training.distributed_port=$[${RANDOM}%30000+30000]"
# 
# # # baseline
# # PREFIX=d2v2/full_swp2_baseline PYTHONPATH=$(pwd) $d2v2_full_cmd $port &
#
# ema combos
# PREFIX=d2v2/full_swp2_ema PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_decay=0.999 model.ema_end_decay=0.99995,0.99999,1.0 model.ema_anneal_end_step=10000 &
# PREFIX=d2v2/full_swp2_ema PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_decay=0.9999 model.ema_end_decay=0.99995  model.ema_anneal_end_step=10000 &
# PREFIX=d2v2/full_swp2_ema PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.ema_decay=0.9999 model.ema_end_decay=0.99999,0.999999,1.0  model.ema_anneal_end_step=10000,25000,50000 &
# 
# # clone and mask combos
# PREFIX=d2v2/full_swp2_clone_mask PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=8 dataset.batch_size=4 task.mask_prob=0.12,0.24,0.36 task.mask_multiple_length=2 &
# PREFIX=d2v2/full_swp2_clone_mask PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=16 dataset.batch_size=2 task.mask_prob=0.12,0.24,0.36 task.mask_multiple_length=2 &
# PREFIX=d2v2/full_swp2_clone_mask PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=32 dataset.batch_size=1 task.mask_prob=0.12,0.24,0.36 task.mask_multiple_length=2 &
# 
# PREFIX=d2v2/full_swp2_clone_mask PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=32,48,64 dataset.batch_size=1 task.mask_prob=0.48,0.60 task.mask_multiple_length=2 &
# PREFIX=d2v2/full_swp2_clone_mask PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=8,12,16 dataset.batch_size=4 task.mask_prob=0.48,0.60 task.mask_multiple_length=2 &
# 
# # mask combo
# PREFIX=d2v2/full_swp2_mask PYTHONPATH=$(pwd) $d2v2_full_cmd $port task.mask_prob=0.02,0.04,0.12 task.mask_multiple_length=1,4,8 &
# PREFIX=d2v2/full_swp2_mask PYTHONPATH=$(pwd) $d2v2_full_cmd $port task.mask_prob=0.02,0.12 task.mask_multiple_length=2 &


# ==================== d2v2 full sweep round 3

d2v1_full_cmd="nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_local_lr.sh +run_config=slurm_2_aws"
port="distributed_training.distributed_port=$[${RANDOM}%30000+30000]"

# PREFIX=d2v2/full_swp3_baseline_d2v1 PYTHONPATH=$(pwd) $d2v1_full_cmd $port optimization.max_update=400000 lr_scheduler.warmup_updates=4000 distributed_training.distributed_world_size=16 &
# PREFIX=d2v2/full_swp3_baseline_d2v1_bs32 PYTHONPATH=$(pwd) $d2v1_full_cmd $port optimization.max_update=400000 lr_scheduler.warmup_updates=4000 distributed_training.distributed_world_size=16 dataset.batch_size=32 +task.include_index=True +task.skip_masking=True &


d2v2_full_cmd="nohup python fairseq_cli/hydra_train.py -m --config-dir examples/data2vec/config/text/pretraining --config-name base_dual_mae_conv_v3_aws hydra/launcher=depedency_submitit_slurm +next_script=examples/data2vec/scripts/text/finetune_all_fair_aws_local_lr.sh +run_config=slurm_2_aws"
port="distributed_training.distributed_port=$[${RANDOM}%30000+30000]"

# # baseline
# PREFIX=d2v2/full_swp3_baseline PYTHONPATH=$(pwd) $d2v2_full_cmd $port &
# 
# # Guassian mask embedding
#PREFIX=d2v2/full_swp3_maskemb PYTHONPATH=$(pwd) $d2v2_full_cmd $port +model.mae_use_rand_emb=True +model.mae_mask_noise_std=0,0.01,0.1 &
# 
# # lr combo
# PREFIX=d2v2/full_swp3_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.encoder.lr_float=8e-5 optimizer.groups.decoder.lr_float=8e-4 &
# PREFIX=d2v2/full_swp3_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.encoder.lr_float=9e-5 optimizer.groups.decoder.lr_float=9e-4 &
# PREFIX=d2v2/full_swp3_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.encoder.lr_float=2e-4 optimizer.groups.decoder.lr_float=2e-3 &
# PREFIX=d2v2/full_swp3_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-3 &

# # clone and mask combos
# PREFIX=d2v2/full_swp3_clone_mask PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=2 dataset.batch_size=16 task.mask_prob=0.24,0.36,0.48 &
# PREFIX=d2v2/full_swp3_clone_mask PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=4 dataset.batch_size=8 task.mask_prob=0.24,0.36,0.48 &
# PREFIX=d2v2/full_swp3_clone_mask PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=8 dataset.batch_size=4 task.mask_prob=0.24,0.36,0.48 &
# PREFIX=d2v2/full_swp3_clone_mask PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=16 dataset.batch_size=2 task.mask_prob=0.24,0.36,0.48 &

# # dec lr/grp/kernel/res
# PREFIX=d2v2/full_swp3_dec PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_layers=3,4,5,6,7 model.mae_conv_decoder_groups=1 model.mae_conv_decoder_kernel=9 model.mae_conv_decoder_residual=True,False &
# PREFIX=d2v2/full_swp3_dec PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_layers=3,5 model.mae_conv_decoder_groups=2,4 model.mae_conv_decoder_kernel=9 model.mae_conv_decoder_residual=False &
# PREFIX=d2v2/full_swp3_dec PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.mae_conv_decoder_layers=3,5 model.mae_conv_decoder_groups=1,2 model.mae_conv_decoder_kernel=5,7 model.mae_conv_decoder_residual=False &

# ## 
# ## # lr + topk
# ## PREFIX=d2v2/full_swp3_lr_topk PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.encoder.lr_float=1e-4 optimizer.groups.decoder.lr_float=1e-3 model.average_top_k_layers=11,10 &
# ## PREFIX=d2v2/full_swp3_lr_topk PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-3 model.average_top_k_layers=12,11,10 &
# ## PREFIX=d2v2/full_swp3_lr_topk PYTHONPATH=$(pwd) $d2v2_full_cmd $port optimizer.groups.encoder.lr_float=5e-4 optimizer.groups.decoder.lr_float=5e-3 model.average_top_k_layers=12,11,10 &
# ## 
# ## # clone and lr
# ## PREFIX=d2v2/full_swp3_clone_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=2 dataset.batch_size=16 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-3 &
# ## PREFIX=d2v2/full_swp3_clone_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=2 dataset.batch_size=16 optimizer.groups.encoder.lr_float=1e-4 optimizer.groups.decoder.lr_float=1e-3 &
# ## PREFIX=d2v2/full_swp3_clone_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=2 dataset.batch_size=16 optimizer.groups.encoder.lr_float=7e-5 optimizer.groups.decoder.lr_float=7e-4 &
# ## PREFIX=d2v2/full_swp3_clone_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=4 dataset.batch_size=8 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-3 &
# ## PREFIX=d2v2/full_swp3_clone_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=4 dataset.batch_size=8 optimizer.groups.encoder.lr_float=1e-4 optimizer.groups.decoder.lr_float=1e-3 &
# ## PREFIX=d2v2/full_swp3_clone_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=4 dataset.batch_size=8 optimizer.groups.encoder.lr_float=7e-5 optimizer.groups.decoder.lr_float=7e-4 &
# ## PREFIX=d2v2/full_swp3_clone_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=8 dataset.batch_size=4 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-3 &
# ## PREFIX=d2v2/full_swp3_clone_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=8 dataset.batch_size=4 optimizer.groups.encoder.lr_float=1e-4 optimizer.groups.decoder.lr_float=1e-3 &
# ## PREFIX=d2v2/full_swp3_clone_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=8 dataset.batch_size=4 optimizer.groups.encoder.lr_float=7e-5 optimizer.groups.decoder.lr_float=7e-4 &
# ## 
# ## PREFIX=d2v2/full_swp3_clone_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=16 dataset.batch_size=4 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-3 &
# ## PREFIX=d2v2/full_swp3_clone_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=32 dataset.batch_size=2 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-3 &
# ## PREFIX=d2v2/full_swp3_clone_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=64 dataset.batch_size=1 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-3 &
# ## 
# ## PREFIX=d2v2/full_swp3_clone_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=32 dataset.batch_size=4 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-3 &
# ## PREFIX=d2v2/full_swp3_clone_lr PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.clone_batch=64 dataset.batch_size=2 optimizer.groups.encoder.lr_float=3e-4 optimizer.groups.decoder.lr_float=3e-3 &
# ## 
# ## # target normalization
# ## PREFIX=d2v2/full_swp3_tgtnorm PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.layer_norm_target_layer=true model.instance_norm_target_layer=false model.batch_norm_target_layer=false model.instance_norm_targets=false model.layer_norm_targets=false,true &
# ## PREFIX=d2v2/full_swp3_tgtnorm PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.layer_norm_target_layer=false model.instance_norm_target_layer=true model.batch_norm_target_layer=false model.instance_norm_targets=false model.layer_norm_targets=false,true &
# ## PREFIX=d2v2/full_swp3_tgtnorm PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.layer_norm_target_layer=false model.instance_norm_target_layer=false model.batch_norm_target_layer=true model.instance_norm_targets=false model.layer_norm_targets=false,true &
# ## PREFIX=d2v2/full_swp3_tgtnorm PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.layer_norm_target_layer=false model.instance_norm_target_layer=true model.batch_norm_target_layer=false model.instance_norm_targets=true model.layer_norm_targets=false &
# ## PREFIX=d2v2/full_swp3_tgtnorm PYTHONPATH=$(pwd) $d2v2_full_cmd $port model.layer_norm_target_layer=false model.instance_norm_target_layer=false model.batch_norm_target_layer=true model.instance_norm_targets=true model.layer_norm_targets=false &
# ## 
